{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OWG9i8TGRvS2"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from textstat import textstat\n",
        "import re\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "id": "gZchQtn_UWWj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('processed_essays.csv')\n"
      ],
      "metadata": {
        "id": "HPIDkI-HUXZG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# text cleaning"
      ],
      "metadata": {
        "id": "ZAIqQ5QiUZBM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'\\d+', '', text)  # Remove digits\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n",
        "    tokens = word_tokenize(text)\n",
        "    tokens = [word for word in tokens if word not in stopwords.words('english')]\n",
        "    return ' '.join(tokens)"
      ],
      "metadata": {
        "id": "bkZA4BK9UeGP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['cleaned_essay'] = df['essay'].apply(clean_text)"
      ],
      "metadata": {
        "id": "zYeU_ibaUipr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['flesch_reading_ease'] = df['cleaned_essay'].apply(lambda x: textstat.flesch_reading_ease(x))\n",
        "df['gunning_fog'] = df['cleaned_essay'].apply(lambda x: textstat.gunning_fog(x))\n",
        "df['avg_sentence_length'] = df['cleaned_essay'].apply(lambda x: np.mean([len(sentence.split()) for sentence in nltk.sent_tokenize(x)]))"
      ],
      "metadata": {
        "id": "6fha9ySIUl5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TF-IDF features\n"
      ],
      "metadata": {
        "id": "yLSAk_4jUoxm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tfidf = TfidfVectorizer(max_features=100)\n",
        "tfidf_matrix = tfidf.fit_transform(df['cleaned_essay'])\n",
        "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf.get_feature_names_out())"
      ],
      "metadata": {
        "id": "iQlE5E2IUq3p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.concat([df, tfidf_df], axis=1)\n"
      ],
      "metadata": {
        "id": "VeLJpwLuUuG9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}